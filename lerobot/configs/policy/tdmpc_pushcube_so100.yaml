# @package _global_

seed: 19123
dataset_repo_id: jpata/so100_pushcube_sim_cur0

training:
  offline_steps: 0

  # Offline training dataloader
  num_workers: 1

  batch_size: 128
  grad_clip_norm: 10.0
  lr: 1e-4

  eval_freq: 10000
  log_freq: 1
  save_freq: 50000

  online_steps: 1000000
  online_rollout_n_episodes: 10
  online_rollout_batch_size: 10
  online_steps_between_rollouts: 1000
  online_sampling_ratio: 1.0
  online_env_seed: 10000
  online_buffer_capacity: 40000
  online_buffer_seed_size: 0
  do_online_rollout_async: false

  #this is not used right now
  delta_timestamps:
    observation.environment_state: "[i / ${fps} for i in range(${policy.horizon} + 1)]"
    observation.state: "[i / ${fps} for i in range(${policy.horizon} + 1)]"
    action: "[i / ${fps} for i in range(${policy.horizon})]"
    next.reward: "[i / ${fps} for i in range(${policy.horizon})]"

  #we use this instead
  delta_timestamp: "[i / ${fps} for i in range(${policy.horizon} + 1)]"
  keys_to_get:
    - observation.environment_state
    - observation.state
    - action
    - next.reward

policy:
  name: tdmpc

  pretrained_model_path:

  # Input / output structure.
  n_action_repeats: 1
  horizon: 32
  n_action_steps: 32

  input_shapes:
    observation.environment_state: [6]
    observation.state: ["${env.state_dim}"]
  output_shapes:
    action: ["${env.action_dim}"]

  # Normalization / Unnormalization
  # input_normalization_modes:
  #   observation.state: min_max
  # output_normalization_modes:
  #   action: min_max

  # Architecture / modeling.
  # Neural networks.
  image_encoder_hidden_dim: 32
  state_encoder_hidden_dim: 256
  latent_dim: 128
  q_ensemble_size: 5
  mlp_dim: 512
  # Reinforcement learning.
  discount: 0.98

  # Inference.
  use_mpc: true
  cem_iterations: 6
  max_std: 2.0
  min_std: 0.05
  n_gaussian_samples: 512
  n_pi_samples: 51
  uncertainty_regularizer_coeff: 1.0
  n_elites: 50
  elite_weighting_temperature: 0.5
  gaussian_mean_momentum: 0.1

  # Training and loss computation.
  max_random_shift_ratio: 0.0476
  # Loss coefficients.
  reward_coeff: 0.5
  expectile_weight: 0.9
  value_coeff: 0.1
  consistency_coeff: 20.0
  advantage_scaling: 3.0
  pi_coeff: 0.5
  temporal_decay_coeff: 0.5
  # Target model.
  target_model_momentum: 0.995
